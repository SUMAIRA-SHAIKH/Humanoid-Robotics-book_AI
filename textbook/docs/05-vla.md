---
title: Module 4 — Vision-Language-Action (VLA)
slug: /vla
sidebar_position: 5
---

# Vision-Language-Action (VLA)

Combining LLMs with perception and robot actions.

## Elements
- **Voice-to-Action**: convert speech to commands (OpenAI Whisper or similar).
- **Cognitive Planning**: LLM translates "Clean the room" → sequence of robot actions.
- **Perception**: object detection and recognition to act on the environment.

## Mini project
- Build a pipeline: speech → LLM → ROS 2 action sequence → simulated execution.
